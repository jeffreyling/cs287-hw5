
\documentclass[11pt]{article}

\usepackage{common}
\title{HW5: Named Entity Recognition}
\author{Jeffrey Ling \\ jling@college.harvard.edu \and Rohil Prasad \\ rprasad@college.harvard.edu} 
\begin{document}

\maketitle{}
\section{Introduction}

In this assignment, we tackle the problem of Named Entity Recognition (NER). The goal of this task is to construct a model that can pick out and determine the type of important named entities in a given text. For example, in the sentence ``I went to Disney World.'', the block ``Disney World'' should be labeled as a special place. 

We implement and discuss several approaches to NER in this paper, all trained on a portion of the CONLL 2003 Shared Task dataset. These include a standard Hidden Markov Model (HMM), a neural Maximum Entropy Markov Model (MEMM), and a structured perceptron. To label entities given our model outputs, we implement the Viterbi algorithm and the forward-backward algorithm. 

Furthermore, unlike previous assignments, feature selection is especially important for this problem. We implement several of the features described in in [Tjong Kim Sang paper] as well. 

In Section 2, we give a formal description of our problem and establish our notation. In Section 3, we give detailed descriptions of the algorithms used for training and evaluation. In Section 4, we present our experimental results. In Section 5, we discuss our findings. 

\section{Problem Description}

We treat this as a tagging problem, similarly to HW2. To capture the named entities, which can span multiple words, we use a system called BIO tagging. A word tagged with an $I$ tag @TODO Explain BIO tagging? 

Our raw data consists of pairs $(w_i, t_i) \in \mcV \times \mcT$ for $i = 1, \dots, N$, where $N$ is our data size, where $w_i \in \mcV$ is a word in our corpus and $t_i$ is its corresponding BIO tag.

\subsection{Feature Extraction}

We construct a feature representation of each word $w$ as follows. 

First, construct the vocabulary $\mcV$ of the training and testing sets. To keep this from growing too large, we only use lower-case words. Furthermore, we map any uncommon words to a special ``UNK'' word. A word is considered to be uncommon if it is not listed in the publically available Glove word embeddings. A word $w$ then can be associated with a sparse one-hot feature vector $\boldj \in \{0,1\}^{|\mcV|}$, with $1$ at the index $j \in \{1, 2, \dots, |\mcV|\}$ corresponding to $w$ in the vocabulary. 

Next, we construct a set $\mathcal{F}$ of features. We implement these based on papers from the CONLL 2003 shared task, but not all of them are used in our final models. The full set includes
\begin{itemize}
  \item The lemma of a word, as given by the NLTK WordNet Lemmatizer. 
  \item The part of speech of a word as given by the NLTK averaged perceptron POS tagger. 
  \item All possible substrings of a word. 
  \item Capitalization information (is all caps, starts with a capitalized letter, or contains a capitalized letter) of a word.
\end{itemize}

We also make available the features (as given above) of surrounding words in an arbitrary window size. Each of these features has an index in $\mathcal{F}$, so we can associate a word $w$ to a $0-1$ vector in $\{0,1\}^{|\mathcal{F}|}$, where a $1$ at index $j$ indicates it has the feature at index $j$ and a $0$ indicates it does not have this feature. 

Our goal is thus to learn a function $f: \{0,1\}^{|\mcV|} \times \{0,1\}^{|\mathcal{F}|} \to \mathbb{R}^{|\mcT|}$ that takes in a word and its associated features and outputs a prediction vector $\hat{\boldy}$ where $\hat{y}_k$ is the log probability that the word has tag $k$. 

\subsection{Practice}

In practice, we represent $w$ with a vector of indexes, namely its index in $\mcV$ and a concatenated vector of all of its feature indicies in $\mathcal{F}$. To ensure that our feature representation of our data forms a matrix for Torch to work with, we pad the vectors with a special ``padding feature'' that has index $1$ in $\mathcal{F}$ to make them all the same length. 

\section{Model and Algorithms}

Here you specify the model itself. This section should formally
describe the model used to solve the task proposed in the previous
section. This section should try to avoid introducing new vocabulary
or notation, when possible use the notation from the previous section.
Feel free to use the notation from class, but try to make the note
understandable as a standalone piece of text.

This section is also a great place to include other material that
describes the underlying structure and choices of your model, for
instance here are some example tables and algorithms from full
research papers:



\begin{itemize}
\item diagrams of your model,

  \begin{center}
    \includegraphics[width=0.4\textwidth]{network}
  \end{center}
\item feature tables,

  \begin{center}
    \begin{tabular}{@{}lll@{}}
      \toprule
      &\multicolumn{2}{c}{Mention Features  } \\
      & Feature & Value Set\\
      \midrule
      & Mention Head & $\mcV$ \\
      & Mention First Word & $\mcV$ \\
      & Mention Last Word & $\mcV$ \\
      & Word Preceding Mention & $\mcV$ \\
      & Word Following Mention & $\mcV$\\
      & \# Words in Mention & $\{1, 2, \ldots \}$ \\
      & Mention Type & $\mathcal{T}$ \\
      \bottomrule
    \end{tabular}
  \end{center}

\item pseudo-code,

  \begin{algorithmic}[1]
    \Procedure{Linearize}{$x_1\ldots x_N$, $K$, $g$}
    \State{$B_0 \gets \langle (\langle \rangle, \{1, \ldots, N\}, 0, \boldh_0, \mathbf{0})  \rangle$}
    \For{$m = 0, \ldots, M-1$ }
    \For{$k = 1, \ldots, |B_m|$}
    \For{$i \in \mcR$}
    \State{$(y, \mcR, s, \boldh) \gets \mathrm{copy}(B_m^{(k)})$}
    \For{word $w$ in phrase $x_i$}
    \State{$y \gets y $ append $w$ }
    \State{$s \gets s + \log q(w, \boldh) $ }
    \State{$\boldh \gets \delta(w, \boldh)$}
    \EndFor{}
    \State{$B_{m+|w_i|} \gets B_{m+|w_i|} + (y, \mcR - i, s,   \boldh)$}
    \State{keep top-$K$ of $B_{m+|w_i|}$ by $f(x, y) + g(\mcR)$}
    \EndFor{}
    \EndFor{}
    \EndFor{}
    \State{\Return{$B_{M}^{(k)}$}}
    \EndProcedure{}
  \end{algorithmic}

\end{itemize}


\section{Experiments}

Finally we end with the experimental section. Each assignment will make clear the main experiments and baselines that you should run. For these experiments you should present a main results table. Here we give a sample Table~\ref{tab:results}. In addition to these results you should describe in words what the table shows and the relative performance of the models.

Besides the main results we will also ask you to present other results
comparing particular aspects of the models. For instance, for word
embedding experiments, we may ask you to show a chart of the projected
word vectors. This experiment will lead to something like
Figure~\ref{fig:clusters}. This should also be described within the
body of the text itself.


\begin{table}[h]
\centering
\begin{tabular}{llr}
 \toprule
 Model &  & Acc. \\
 \midrule
 \textsc{Baseline 1} & & 0.45\\
 \textsc{Baseline 2} & & 2.59 \\
 \textsc{Model 1} & & 10.59  \\
 \textsc{Model 2} & &13.42 \\
 \textsc{Model 3} & & 7.49\\
 \bottomrule
\end{tabular}
\caption{\label{tab:results} Table with the main results.}
\end{table}


\begin{figure}
  \centering
  \includegraphics[width=6cm]{cluster_viz}
  \caption{\label{fig:clusters} Sample qualitative chart.}
\end{figure}


\section{Conclusion}

End the write-up with a very short recap of the main experiments and the main results. Describe any challenges you may have faced, and what could have been improved in the model.

\bibliographystyle{apalike}
\bibliography{writeup}

\end{document}
